{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bceb9af-14d1-4116-86c0-04ea8cd8b46e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# SQL Validation and correction experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac3f1f9-6967-4ff5-a112-061bf392f7f8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%pip install requests\n",
    "%pip install sqlglot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66912adf-9913-4cce-8dea-b616ffdd072b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Test SQL Validation using sqlglot library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8b3b8-df0b-4cb2-abe5-9a82545cd192",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import sqlglot\n",
    "\n",
    "\n",
    "\n",
    "def validate_sql_statement(sql_statement):\n",
    "    try:\n",
    "        # Attempt to parse and validate the SQL statement\n",
    "        sqlglot.parse_one(sql_statement, dialect=\"spark\")\n",
    "        print(f\"{sql_statement}: Parsed OK\")\n",
    "        return True\n",
    "    except sqlglot.errors.ParseError as e:\n",
    "        print(f\"SQL Parse Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# this is correct\n",
    "validate_sql_statement(\"SELECT * FORM table_name\")\n",
    "\n",
    "# why doesn't this fail?\n",
    "validate_sql_statement(\"SELCT * FORM table_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e1b030-78ba-45bc-ad20-f19fbce5caeb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Attempt to execute SQL to test validity\n",
    "https://sqlglot.com/sqlglot.html#parser-errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ad18d-e35f-4931-a61a-33d67e2f9af8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from sqlglot.executor import execute\n",
    "\n",
    "tables = {\n",
    "    \"sushi\": [\n",
    "        {\"id\": 1, \"price\": 1.0},\n",
    "        {\"id\": 2, \"price\": 2.0},\n",
    "        {\"id\": 3, \"price\": 3.0},\n",
    "    ],\n",
    "    \"order_items\": [\n",
    "        {\"sushi_id\": 1, \"order_id\": 1},\n",
    "        {\"sushi_id\": 1, \"order_id\": 1},\n",
    "        {\"sushi_id\": 2, \"order_id\": 1},\n",
    "        {\"sushi_id\": 3, \"order_id\": 2},\n",
    "    ],\n",
    "    \"orders\": [\n",
    "        {\"id\": 1, \"user_id\": 1},\n",
    "        {\"id\": 2, \"user_id\": 2},\n",
    "    ],\n",
    "}\n",
    "\n",
    "execute(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "      o.user_id,\n",
    "      SUM(s.price) AS price\n",
    "    FROM orders o\n",
    "    JOIN order_items i\n",
    "      ON o.id = i.order_id\n",
    "    JOIN sushi s\n",
    "      ON i.sushi_id = s.id\n",
    "    GROUP BY o.user_id\n",
    "    \"\"\",\n",
    "    tables=tables\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38182af7-c720-4930-9eb4-d04ed2868b7c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "execute(\n",
    "    \"\"\"\n",
    "    SELECT * FOM orders\n",
    "    \"\"\",\n",
    "    tables=tables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc53cb-c757-41bd-9e5f-0cfb98cb1dfd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab4c771e-c3f5-47d5-9b8e-748ee3baec94",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Class to check and correct SQL using OpenAI\n",
    "Using a simple prompt and a 3 tries loop to check and potentially update the SQL given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc74f055-6fc8-4952-8738-9bfab60fc413",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-15T08:36:33.0056439Z",
       "execution_start_time": "2025-05-15T08:36:32.764344Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "cc727b9e-ac26-40d2-b6b9-7a2f622c673d",
       "queued_time": "2025-05-15T08:36:32.7630538Z",
       "session_id": "502abb4d-e265-4189-af4e-aa93c0d07d25",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 15,
       "statement_ids": [
        15
       ]
      },
      "text/plain": [
       "StatementMeta(, 502abb4d-e265-4189-af4e-aa93c0d07d25, 15, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sqlglot\n",
    "import requests\n",
    "import os\n",
    "\n",
    "class SQLValidator:\n",
    "    def __init__(self, azure_openai_endpoint, azure_openai_api_key, model=\"gpt-4\", max_attempts=5):\n",
    "        \"\"\"\n",
    "        Initialize the SQLValidator with Azure OpenAI endpoint, API key, and max attempts.\n",
    "        \"\"\"\n",
    "        self.azure_openai_endpoint = azure_openai_endpoint\n",
    "        self.azure_openai_api_key = azure_openai_api_key\n",
    "        self.model = model\n",
    "        self.max_attempts = max_attempts\n",
    "\n",
    "    def validate_sql(self, sql_statement):\n",
    "        \"\"\"\n",
    "        Validate the SQL statement using sqlglot.\n",
    "        Returns True if valid, otherwise False.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Attempt to parse and validate the SQL statement\n",
    "            sqlglot.parse_one(sql_statement, dialect=\"spark\")\n",
    "            return True\n",
    "        except sqlglot.errors.ParseError as e:\n",
    "            print(f\"SQL Parse Error: {e}\")\n",
    "            return False\n",
    "\n",
    "    def amend_sql(self, invalid_sql):\n",
    "        \"\"\"\n",
    "        Send the invalid SQL to Azure OpenAI for amendment.\n",
    "        Returns the amended SQL statement.\n",
    "        \"\"\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"api-key\": self.azure_openai_api_key\n",
    "        }\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant that fixes SQL syntax errors. Only return the SQL and no other content\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Fix the following SQL: {invalid_sql}\"}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        response = requests.post(self.azure_openai_endpoint, json=payload, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            amended_sql = response.json().get(\"choices\", [])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "            return amended_sql\n",
    "        else:\n",
    "            raise Exception(f\"Azure OpenAI API Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "    def process_sql(self, sql_statement):\n",
    "        \"\"\"\n",
    "        Validate and amend the SQL statement until it passes validation or max attempts are reached.\n",
    "        \"\"\"\n",
    "        attempts = 0\n",
    "        while attempts < self.max_attempts:\n",
    "            if self.validate_sql(sql_statement):\n",
    "                print(\"SQL is valid!\")\n",
    "                return sql_statement\n",
    "\n",
    "            print(f\"Attempt {attempts + 1}/{self.max_attempts}: SQL is invalid. Sending to Azure OpenAI for amendment...\")\n",
    "            sql_statement = self.amend_sql(sql_statement)\n",
    "            print(f\"Amended SQL: {sql_statement}\")\n",
    "            attempts += 1\n",
    "\n",
    "        raise Exception(\"Maximum attempts reached. SQL could not be validated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60eb30b-29e9-42f7-b69f-cd0221e69367",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Grab keys from the environment\n",
    "Not sure how to do this in a Fabric notebook. More work needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab434fe0-e103-41f9-bb7b-f8b9b8938e81",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "if not AZURE_OPENAI_ENDPOINT or not AZURE_OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"Please set the AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY environment variables.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b2997-9ef8-4892-8ed7-ec214b4cf53d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## My Keys to OpenAI\n",
    "Need to remove these and make environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c1f392-e315-404e-ae89-3e76900fdc91",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "AZURE_OPENAI_ENDPOINT = \"https://MY-ENDPOINT.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview\"\n",
    "AZURE_OPENAI_API_KEY = \"XXXX\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e23414b-d00b-4271-bf98-96c8decf6206",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Validate and correct the given SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4443d077-87d2-49d3-9152-49ef6dbf58bb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "sql_validator = SQLValidator(AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, max_attempts=3)\n",
    "\n",
    "sql = \"SELECT * FORM table_name\"  # Example of invalid SQL\n",
    "try:\n",
    "    valid_sql = sql_validator.process_sql(sql)\n",
    "    print(f\"Final Valid SQL: {valid_sql}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc1f6c-9a67-43b2-b7ce-8a5472fae559",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Using Spark SQL to get the lakehouse schema\n",
    "Can we query the target lakehouse schema so we can potentially use this to ground the OpenAI query when fixing SQL errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b83860-0f78-41d7-9330-32d4bef80ca4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils\n",
    "\n",
    "# abfss://BEAR_DEV@onelake.dfs.fabric.microsoft.com/ben_testing.Lakehouse/Tables/tgt/organizationwebsite\n",
    "\n",
    "tables_path = \"abfss://BEAR_DEV@onelake.dfs.fabric.microsoft.com/ben_testing.Lakehouse/Tables/tgt/\"\n",
    "tables = mssparkutils.fs.ls(tables_path)\n",
    "for table in tables:\n",
    "    print(table.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce7a5ad-49b6-4c74-8e7c-9e98efc36131",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Query the lakehouse and generate a JSON schema document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a410c2-aafc-4fe1-a39a-ffbcd1fd7db0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from notebookutils import mssparkutils\n",
    "\n",
    "def getLakehouseSchema(lakehousePath):\n",
    "    tables = mssparkutils.fs.ls(lakehousePath)\n",
    "\n",
    "    table_schemas = {}\n",
    "\n",
    "    for table in tables:\n",
    "        table_name = table.name\n",
    "        if \".\" not in table_name:  # Filter out non-table items based on the absence of a file extension\n",
    "            try:\n",
    "                df = spark.read.format(\"delta\").load(f\"{lakehousePath}{table_name}\")\n",
    "                schema = [field.name for field in df.schema.fields]\n",
    "                table_schemas[table_name] = schema\n",
    "            except Exception as e:\n",
    "                table_schemas[table_name] = f\"Error reading schema: {str(e)}\"\n",
    "\n",
    "    # Convert to JSON string\n",
    "    #grounding_json = json.dumps(table_schemas, indent=4)\n",
    "    return table_schemas #grounding_json\n",
    "\n",
    "schema = getLakehouseSchema(\"abfss://BEAR_DEV@onelake.dfs.fabric.microsoft.com/ben_testing.Lakehouse/Tables/Party/\")\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455aac40-e4e4-487e-943e-8b9da44de813",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Test to see if we can use this output with SQLGLOT\n",
    "Looks like I need some data to really use this method effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ea6d2-e258-4ba7-ae60-972b1a5877c7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from sqlglot.executor import execute\n",
    "\n",
    "tables = {\n",
    "    \"organizationcontactdetail\": [\n",
    "        {\"PartyPermId\": 1, \"SourceTypePermId\": 1.0}\n",
    "    ],\n",
    "    \"order_items\": [\n",
    "        {\"sushi_id\": 1, \"order_id\": 1},\n",
    "        {\"sushi_id\": 1, \"order_id\": 1},\n",
    "        {\"sushi_id\": 2, \"order_id\": 1},\n",
    "        {\"sushi_id\": 3, \"order_id\": 2},\n",
    "    ],\n",
    "    \"orders\": [\n",
    "        {\"id\": 1, \"user_id\": 1},\n",
    "        {\"id\": 2, \"user_id\": 2},\n",
    "    ],\n",
    "}\n",
    "\n",
    "execute(\"SELECT PartyPermId, SourceTypePermId FROM organizationcontactdetail\",tables=tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5def084-2707-403a-87c3-d70a24de484c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Use the grounding_json with OpenAI\n",
    "\n",
    "Showing some errors, because we need to firstly test for correct SQL and then to validate that the SQL matches the schema. There needs to be some thought about whether the validation part belongs in SQLGLOT or OpenAI\n",
    "\n",
    "May need to be tidied up.\n",
    "\n",
    "Can we cache the schema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c67aa72-824f-49a9-b709-20a98e40a6bc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import sqlglot\n",
    "import requests\n",
    "import os\n",
    "\n",
    "class GroundedSQLValidator:\n",
    "    def __init__(self, azure_openai_endpoint, azure_openai_api_key, model=\"gpt-4\", max_attempts=5):\n",
    "        \"\"\"\n",
    "        Initialize the GroundedSQLValidator with Azure OpenAI endpoint, API key, and max attempts.\n",
    "        \"\"\"\n",
    "        self.azure_openai_endpoint = azure_openai_endpoint\n",
    "        self.azure_openai_api_key = azure_openai_api_key\n",
    "        self.model = model\n",
    "        self.max_attempts = max_attempts\n",
    "\n",
    "    def validate_sql(self, sql_statement):\n",
    "        \"\"\"\n",
    "        Validate the SQL statement using sqlglot.\n",
    "        Returns True if valid, otherwise False.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Attempt to parse and validate the SQL statement\n",
    "            sqlglot.parse_one(sql_statement, dialect=\"spark\")\n",
    "            return True\n",
    "        except sqlglot.errors.ParseError as e:\n",
    "            print(f\"SQL Parse Error: {e}\")\n",
    "            return False\n",
    "\n",
    "    def amend_sql(self, invalid_sql):\n",
    "        \"\"\"\n",
    "        Send the invalid SQL to Azure OpenAI for amendment.\n",
    "        Returns the amended SQL statement.\n",
    "        \"\"\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"api-key\": self.azure_openai_api_key\n",
    "        }\n",
    "        lakehouse_schema = getLakehouseSchema(\"abfss://BEAR_DEV@onelake.dfs.fabric.microsoft.com/ben_testing.Lakehouse/Tables/Party/\")\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant that fixes SQL syntax errors. Only return the SQL and no other content\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Using this JSON representation of the SQL schema {lakehouse_schema}, fix the following SQL to conform to the given schema: {invalid_sql}\"}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        response = requests.post(self.azure_openai_endpoint, json=payload, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            amended_sql = response.json().get(\"choices\", [])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "            return amended_sql\n",
    "        else:\n",
    "            raise Exception(f\"Azure OpenAI API Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "    def process_sql(self, sql_statement):\n",
    "        \"\"\"\n",
    "        Validate and amend the SQL statement until it passes validation or max attempts are reached.\n",
    "        \"\"\"\n",
    "        attempts = 0\n",
    "        while attempts < self.max_attempts:\n",
    "            if self.validate_sql(sql_statement):\n",
    "                print(\"SQL is valid!\")\n",
    "                #return sql_statement\n",
    "            # now test it conforms to the schema\n",
    "            print(f\"Attempt {attempts + 1}/{self.max_attempts}: SQL is invalid. Sending to Azure OpenAI for amendment...\")\n",
    "            sql_statement = self.amend_sql(sql_statement)\n",
    "            print(f\"Amended SQL: {sql_statement}\")\n",
    "            attempts += 1\n",
    "\n",
    "        raise Exception(\"Maximum attempts reached. SQL could not be validated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faff478-4632-4d68-a22d-8594125fa2ce",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Try out the validator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e8259-96b6-41b9-96b9-e230dd9210c9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "sql_validator = GroundedSQLValidator(AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, max_attempts=3)\n",
    "\n",
    "sql = \"SELECT PartyPermId, SourcTypePermId FROM organizationcontactdetail\"  # Example of invalid SQL\n",
    "try:\n",
    "    valid_sql = sql_validator.process_sql(sql)\n",
    "    print(f\"Final Valid SQL: {valid_sql}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "6e70d2e8-746f-484d-a630-dd9b7f73d261",
    "default_lakehouse_name": "ben_testing",
    "default_lakehouse_workspace_id": "3a01cd0d-fb22-4f11-ac92-d0dc5f2a77a8",
    "known_lakehouses": [
     {
      "id": "6e70d2e8-746f-484d-a630-dd9b7f73d261"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
